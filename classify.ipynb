{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import BertTokenizer, BertModel, get_linear_schedule_with_warmup\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading and Preprocessing Scientific Abstracts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory of research articles\n",
    "astronomy_articles=\"./raw_scrape/astronomy.json\"\n",
    "psychology_articles=\"./raw_scrape/psychology.json\"\n",
    "sociology_articles=\"./raw_scrape/sociology.json\"\n",
    "\n",
    "# Reading the JSON files into a pandas dataframe\n",
    "df_astro = pd.read_json(astronomy_articles) \n",
    "df_astro.insert(0, \"category\", \"astronomy\") # Adding a category column with value astronomy \n",
    "df_socio = pd.read_json(sociology_articles)\n",
    "df_socio.insert(0, \"category\", \"sociology\") # Adding a category column with value sociology\n",
    "df_psycho = pd.read_json(psychology_articles)\n",
    "df_psycho.insert(0, \"category\", \"psychology\") # Adding a category column with value psychology\n",
    "\n",
    "df = pd.concat([df_astro, df_socio, df_psycho], axis=0, ignore_index=True) # Combining all the dataframes\n",
    "df = df.rename(columns={0 : \"article\"}) # Changing the name of the column that contains the articles from 0 to \"article\"\n",
    "print(df.groupby(\"category\").describe()) # There are some duplicate articles for reasons unknown. The scraping code ensured that no articles was taken twice unless the API has duplicates articles with different urls. However, each field has above 300 articles\n",
    "df = df.drop_duplicates(subset=\"article\", keep=\"first\") # Keep the first occurrence of a duplicate and removing the second occurence\n",
    "print(df.groupby(\"category\").describe()) # There are some duplicate articles for reasons unknown. The scraping code ensured that no articles was taken twice unless the API has duplicates articles with different urls. However, each field has above 300 articles\n",
    "df['category_num'] = df['category'].map({ # Adding a new column called category_num that represents the field of the article in numerical form. \n",
    "    'astronomy': 0, # Maps all astronomy papers to 0\n",
    "    'sociology': 1, # Maps all sociology papers to 1\n",
    "    'psychology': 2 # Maps all psychology papers to 2\n",
    "})\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating a custom dataset class to preprocess text from human readable language to tokens that BERT uses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Defines a custom dataset class for text classification tasks. \n",
    "Prepares raw text data from the pandas dataframe into the format that BERT requires.\n",
    "\n",
    "Format that BERT requires:\n",
    "Input ID: Integers that represent words in tokens based on a tokenization vocabulary for bert-base-uncased.\n",
    "Attention Mask: A binary list that separates real tokens from padded tokens (Padded tokens are tokens used to make input sequences the same length == 512, if a sentence has less than 512 tokens then padded tokens are added but they do not hold any relevant information so model needs to know these tokens can be ignored).\n",
    "Label: Convert the label encoding into torch tensors.\n",
    "\n",
    "Inherits from PyTorch's `Dataset` class.\n",
    "\n",
    "Attributes:\n",
    "    texts (Pandas df) : Dataframe that contains all the scientific abstracts in strings.\n",
    "    labels (Pandas df) : Dataframe that contains all the scientific abstract labels in string (should be the same length as texts).\n",
    "    tokenizer (BERT Tokenizer instance) : BERT tokenizing object that tokenizes the scientific abstracts based on a tokenization vocabulary.\n",
    "    max_length (int): Maximum length of the input sequences (set to 512 for BERT, as it can handle up to 512 tokens).\n",
    "\"\"\"\n",
    "class TextClassificationDataset(Dataset):\n",
    "\n",
    "    def __init__(self, texts, labels, tokenizer, max_length=512):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length # max length is set to 512 since BERT can only handle up to 512 tokens\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        label = self.labels[idx]\n",
    "        encoding = self.tokenizer(text, return_tensors='pt', max_length=self.max_length, padding='max_length', truncation=True) #Truncation = True means that extracts that creates more than 512 tokens are cut off\n",
    "        return {\n",
    "                'input_ids': encoding['input_ids'].flatten(), \n",
    "                'attention_mask': encoding['attention_mask'].flatten(), \n",
    "                'label': torch.tensor(label)\n",
    "                }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating the machine learning model which uses BERT and a classifying layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Creating the machine learning model based on BERT with a classifying layer.\n",
    "\n",
    "Inherits nn.Module from PyTorch to define the model's architecture and forward pass.\n",
    "\n",
    "Attributes:\n",
    "    bert_model_name (str) : Name of pre-trained BERT model to be used ex.\"bert-base-uncased\".\n",
    "    num_classes (int) : Number of output classes (3 in this case which is Astronomy, Psychology, and Sociology).\n",
    "\"\"\"\n",
    "class BERTClassifier(nn.Module):\n",
    "    def __init__(self, bert_model_name, num_classes):\n",
    "        super(BERTClassifier, self).__init__()\n",
    "        self.bert = BertModel.from_pretrained(bert_model_name) # Loads a pretrained BERT model\n",
    "        self.dropout = nn.Dropout(0.1) # Creates a dropout layer that is connected to the output of the BERT layer and the input of the classifying layer. 1 out of 10 neurons are randomly turned off to prevent overfitting \n",
    "        self.fc = nn.Linear(self.bert.config.hidden_size, num_classes) # A linear neural network layer with input size of 768 neurons (size of BERT's hidden layer) and output size equal to 3 (number of class for labelling)\n",
    "    \"\"\"\n",
    "    Handles the computation from tokenized sequences -> BERT Processing -> dropout -> and classification to one of the three classes.\n",
    "    \n",
    "    Attributes:\n",
    "        input_ids (PyTorch tensor) : Tokenized and encoded IDs of the input text (abstract). Tokenized and encoded by BERT tokenizer.\n",
    "        attention_mask (PyTorch tensor) : A mask returned by BERT Tokenizer to differentiate real tokens and padding tokens.\n",
    "    \"\"\"\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask) # Runs the input through BERT\n",
    "        pooled_output = outputs.pooler_output # Extract the pooler_output from the outputs dict. pooler_output is a vector of fixed size 768 (number of neurons) that represents the sequence of IDs from the input\n",
    "        x = self.dropout(pooled_output) # Applies dropout to the pooled output \n",
    "        logits = self.fc(x) # Passes BERT's output to the classifying head (after applying dropout) for classification. Returns logits which can be passed through a softmax function for probabilities of each output class\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating the training function to fine tune the machine learning model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Training function to train a multi-layered machine learning model based on BERT's architecture\n",
    "\n",
    "Attributes:\n",
    "    model (nn.Module) : Model that will be trained.\n",
    "    data_loader (DataLoader) : A PyTorch DataLoader object to supply batches of training data. DataLoader object contains tokenized and encoded sequences from BERT Tokenizer.\n",
    "    optimizer (torch.optim.Optimizer) : Optimizer used to update the model's parameters based on the computed gradients.\n",
    "    scheduler (torch.optim.lr_scheduler.LambdaLR) : Learning rate scheduler responsible for adjusting the learning rate during training. Learning rate decreases throughout training and this scheduler contains the predefined strategy to decrease the learning rate.\n",
    "    device (str) : Specifies where the model and data will be moved to. Model, input, and labels should be processed on the same hardware. \"cuda\" for GPU and \"cpu\" for cpu.\n",
    "\"\"\"\n",
    "def train(model, data_loader, optimizer, scheduler, device):\n",
    "    model.train() # Sets the model to training mode\n",
    "    for batch_idx, batch in enumerate(data_loader): # Iterates sequence batches in data loader according to batch_size\n",
    "        optimizer.zero_grad() # Zero the gradients from previous batch\n",
    "        input_ids = batch['input_ids'].to(device) \n",
    "        attention_mask = batch['attention_mask'].to(device) \n",
    "        labels = batch['label'].to(device) # Moving the batch data to the specified device\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask) # Forward pass of batch data to obtain output (logits)\n",
    "        loss = nn.CrossEntropyLoss()(outputs, labels) # calculate the loss between the output and the true labels \n",
    "        loss.backward() # Backpropagation to compute the gradient of the loss with respect to the model's current parameters. Gradients are stored as weights to be updated later. \n",
    "        optimizer.step() # Updates the model's parameters according to the calculated gradients during the backward pass\n",
    "        scheduler.step() # Updates the learning rate \n",
    "\n",
    "        # Log the current learning rate\n",
    "        current_lr = scheduler.get_last_lr()[0]  # Get the current learning rate\n",
    "        progress = f\"Batch {batch_idx + 1}/{len(data_loader)}, Loss: {loss.item():.4f}, Learning Rate: {current_lr:.8f}\" # Display the current batch, loss, and learning rate\n",
    "        sys.stdout.write(\"\\r\" + progress) # Dont print a new line but overlap, like a progress bar\n",
    "        sys.stdout.flush()  \n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating the evaluation function to monitor the performance of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Evaluation function to assess the performance of the model based on an evaluation set or training set. Calculates the loss, accuracy, precision, recall, f1-score for the given dataset. \n",
    "\n",
    "Attributes:\n",
    "    model (nn.Module) : Trained PyTorch machine learning model to evaluate\n",
    "    data_loader (DataLoader) : PyTorch DataLoader object that supplies data equal to batch_size for prediction\n",
    "    device (str) : Specifies which hardware device to send the model and data to. \n",
    "\"\"\"\n",
    "def evaluate(model, data_loader, device):\n",
    "    model.eval() # Sets the model to evaluation mode\n",
    "    predictions = [] # Initialize list for predictions\n",
    "    actual_labels = [] # Initialize list for true labels\n",
    "    total_loss = 0.0  # Variable to accumulate loss\n",
    "    \n",
    "    with torch.no_grad():  # Disable gradient computation for evaluation\n",
    "        for batch in data_loader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device) \n",
    "            labels = batch['label'].to(device) # Sending data batches to device\n",
    "            \n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask) # Forward pass of the data through the model \n",
    "            logits = outputs  # Get the raw logits output from the model\n",
    "            loss = nn.CrossEntropyLoss()(logits, labels)  # Calculate loss between prediction label and true label\n",
    "            total_loss += loss.item()  # Accumulate loss for averaging\n",
    "            \n",
    "            _, preds = torch.max(logits, dim=1)  # Get predicted class indices based on the model's output logits. In this case, logits are not passed through a function like softmax because simply selecting the class with the maximum value from the logits tensor is sufficient. \n",
    "            predictions.extend(preds.cpu().tolist()) # Add prediction result to prediction list\n",
    "            actual_labels.extend(labels.cpu().tolist()) # Add true label to true label list\n",
    "    \n",
    "    # Compute the average loss\n",
    "    avg_loss = total_loss / len(data_loader)\n",
    "    \n",
    "    # Calculate accuracy and other classification metrics\n",
    "    accuracy = accuracy_score(actual_labels, predictions)\n",
    "    report = classification_report(actual_labels, predictions) # calculates f1-score, precision, and recall per class\n",
    "\n",
    "    return avg_loss, accuracy, report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating an early stopping class to stop fine tuning when validation loss stops improving (precaution to prevent overfitting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "EarlyStopping class that monitors a validation metric per epoch and stops training early if validation metric does not improve after a certain number of epochs as specific by the patience value.\n",
    "The class saves the model weight with the best validation metric. \n",
    "\n",
    "Attributes:\n",
    "    patience (int): Number of consecutive epochs without improvement before training is stopped. \n",
    "    delta (float): Minimum change in validation metric to qualify as an improvement. \n",
    "    mode (str): Minimize (validation loss) or maximize (validation accuracy) of validation metric.\n",
    "    verbose (bool): Show or hide information such as when validation metric improves or stopping counter increases. \n",
    "\"\"\"\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=3, delta=0, mode='min', verbose=False):\n",
    "\n",
    "        self.patience = patience\n",
    "        self.delta = delta\n",
    "        self.mode = mode\n",
    "        self.verbose = verbose\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "        self.counter = 0\n",
    "        self.best_model_wts = None # save the weights of the best model\n",
    "\n",
    "    def __call__(self, val_loss, model):\n",
    "        \"\"\"\n",
    "        Call this method after each validation to check whether to stop the training.\n",
    "\n",
    "        Attributes: \n",
    "            val_loss (float): The current validation loss.\n",
    "            model (torch.nn.Module): The model being trained. The best model's weights will be saved. \n",
    "        \"\"\"\n",
    "        if self.best_score is None:\n",
    "            self.best_score = val_loss\n",
    "            self.save_best_model(model)\n",
    "        elif (self.mode == 'min' and val_loss < self.best_score - self.delta) or \\\n",
    "             (self.mode == 'max' and val_loss > self.best_score + self.delta):\n",
    "            self.best_score = val_loss\n",
    "            self.save_best_model(model) # Saving the weight of the best model\n",
    "            self.counter = 0\n",
    "        else:\n",
    "            self.counter += 1\n",
    "            if self.verbose: # If verbose is False (don't show information), the print statement will be omitted \n",
    "                print(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "\n",
    "    def save_best_model(self, model):\n",
    "        \"\"\"\n",
    "        Save the model weights.\n",
    "\n",
    "        Arguments:\n",
    "            model (nn.Module) : Weights from this model will be saved\n",
    "        \"\"\"\n",
    "        self.best_model_wts = model.state_dict() # Saving the weights of the best model in best_model_wts attribute of EarlyStopping class object\n",
    "\n",
    "    def restore_best_model(self, model):\n",
    "        \"\"\"\n",
    "        Restore the model weights from the best epoch according to the validation metric\n",
    "\n",
    "        Arguments: \n",
    "            model (nn.Module) : Weights from the best model will be loaded to this model\n",
    "        \"\"\"\n",
    "        if self.best_model_wts is not None:\n",
    "            model.load_state_dict(self.best_model_wts) # Loading the weights of the best model from best_model_wts attribute of EarlyStopping class object"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting up parameters for the machine learning model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up model parameters\n",
    "bert_model_name = 'bert-base-cased' # Base BERT model with 12 layers. Cased meaning the model distinguishes between lowercase and uppercase words which is important for scientific journals where preserving capitalization can bring important insight to domain specific knowledge\n",
    "num_classes = 3 # 3 classification classes (\"Astronomy\", \"Sociology\", \"Psychology\")\n",
    "max_length = 512 # 512 tokens for base BERT\n",
    "batch_size = 16 # 16 is the perfect balance between memory costs, training time, and model performance\n",
    "num_epochs = 30 # 30 epochs is excessive and risks overfitting but EarlyStopping class should prevent that \n",
    "learning_rate = 1e-5 # Quite low learning rate from the beginning because BERT is a powerful model for a simple NLP classifying task such as this where domain specific terminologies are very evident. Therefore, convergence should be quite quick and a low learning rate improves convergence and reduces instability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fine tuning the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initializing the folds\n",
    "skf = StratifiedKFold(n_splits=5, random_state=4, shuffle=True) # Splits the dataset into 5 folds. Using StratifiedKFold so the distribution is equal across all classes. Astronomy papers make up 1/3, Psychology makes up 1/3, and finally Sociology makes up 1/3.\n",
    "folds = skf.split(df[\"article\"], df[\"category_num\"]) # Splits the dataset into 4/5 training data and 1/5 testing data \n",
    "\n",
    "# Empty list to be used later to create the confusion matrix\n",
    "true_labels_confusion = []\n",
    "predicted_labels_confusion = []\n",
    "\n",
    "for i, (fold_train, fold_test) in enumerate(folds, start=1):\n",
    "\n",
    "    train_fold = df.iloc[fold_train] # Training fold\n",
    "    test_fold = df.iloc[fold_test]  # Testing fold\n",
    "\n",
    "    # Separating texts and label from training dataframe\n",
    "    train_texts = train_fold[\"article\"]\n",
    "    train_labels = train_fold[\"category_num\"] \n",
    "\n",
    "    # Splitting training data into training data (3/4) and evaluating data to be used for EarlyStopping (1/4)\n",
    "    train_texts, val_texts, train_labels, val_labels = train_test_split(train_texts, train_labels, test_size=0.25, random_state=4)\n",
    "\n",
    "    # Turning it back into a dataframe\n",
    "    train_texts = pd.DataFrame(train_texts)[\"article\"]\n",
    "    val_texts = pd.DataFrame(val_texts)[\"article\"]\n",
    "    train_labels = pd.DataFrame(train_labels)[\"category_num\"]\n",
    "    val_labels = pd.DataFrame(val_labels)[\"category_num\"]\n",
    "\n",
    "    # Separating texts and label from testing dataframe\n",
    "    test_texts = test_fold[\"article\"]\n",
    "    test_labels = test_fold[\"category_num\"]\n",
    "\n",
    "    # Resetting index for all Dataframes\n",
    "    train_texts = train_texts.reset_index(drop=True) \n",
    "    train_labels = train_labels.reset_index(drop=True) \n",
    "    val_texts = val_texts.reset_index(drop=True) \n",
    "    val_labels = val_labels.reset_index(drop=True) \n",
    "    test_texts = test_texts.reset_index(drop=True) \n",
    "    test_labels = test_labels.reset_index(drop=True) \n",
    "\n",
    "    # Setting up the device and model \n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") \n",
    "    model = BERTClassifier(bert_model_name, num_classes).to(device)\n",
    "\n",
    "    # Initializing tokenizer, dataset, and data loader\n",
    "    tokenizer = BertTokenizer.from_pretrained(bert_model_name)\n",
    "    train_dataset = TextClassificationDataset(train_texts, train_labels, tokenizer)\n",
    "    val_dataset = TextClassificationDataset(val_texts, val_labels, tokenizer, max_length)\n",
    "    test_dataset = TextClassificationDataset(test_texts, test_labels, tokenizer, max_length)\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_dataloader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "    test_dataloader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "\n",
    "    # Setting up optimizer and learning rate scheduler\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "    total_steps = len(train_dataloader) * num_epochs \n",
    "    scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)\n",
    "\n",
    "    # Setting up early stopping\n",
    "    early_stopping = EarlyStopping(patience=5, mode=\"min\", verbose=True) # Patience is quite high but learning rate is very low so it gives the model more time to possibly converge better. \n",
    "\n",
    "    print(f\"Fold {i}\" + \"_\"*100)\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f\"Epoch {epoch + 1}/{num_epochs}\")\n",
    "        train(model, train_dataloader, optimizer, scheduler, device) # Training the model per epoch\n",
    "        val_loss, accuracy, report = evaluate(model, val_dataloader, device) # Evaluate the model at each epoch after training is finished\n",
    "        print(f\"Validation Accuracy: {accuracy:.4f} | Validation Loss: {val_loss:.4f}\") # Printing the Accuracy and Loss based on the validation set\n",
    "        print(report) # Printing the recall, precision, and f1 score\n",
    "        early_stopping(val_loss, model) # Based on the validation loss and patience counter, determine whether training should stop at this epoch\n",
    "        # If early stopping is triggered, exit training loop\n",
    "        if early_stopping.early_stop:\n",
    "            print(f\"Early stopping triggered. Stopping training at Epoch {epoch + 1}/{num_epochs}\") \n",
    "            early_stopping.restore_best_model(model) # Training ends and the model with the best weight is restored to the current model\n",
    "            break\n",
    "\n",
    "    # Setup to assess performance of this fold's model on testing data\n",
    "    # Set model to evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    # Lists to store true and predicted labels\n",
    "    true_labels = []\n",
    "    predicted_labels = []\n",
    "\n",
    "    class_names=[\"Astronomy\", \"Sociology\", \"Psychology\"]\n",
    "\n",
    "    # Disable gradient calculations for faster inference\n",
    "    with torch.no_grad():\n",
    "        for batch in test_dataloader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['label'].to(device) # Putting all the batch data into the hardware device\n",
    "\n",
    "            # Forward pass (Prediction), returns logits\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "\n",
    "            # Uses the returned logits and puts it through a max function to find the class with the highest probability. The class with the highest probability is the prediction based on the input text\n",
    "            _, preds = torch.max(outputs, dim=1)\n",
    "\n",
    "            # Store results\n",
    "            true_labels.extend(labels.cpu().tolist())  # Convert to list and store\n",
    "            predicted_labels.extend(preds.cpu().tolist())  # Convert to list and store\n",
    "\n",
    "    # Appending the true label and predicted label of this fold's model to a list outside of the loop to create an aggregated confusion matrix\n",
    "    true_labels_confusion = true_labels_confusion + true_labels\n",
    "    predicted_labels_confusion = predicted_labels_confusion + predicted_labels\n",
    "\n",
    "    # Print classification report which contains the precision, recall, f1-score, and accuracy of this fold's model performance\n",
    "    print(f\"Performance report for fold {i}\")\n",
    "    print(classification_report(true_labels, predicted_labels, target_names=class_names))\n",
    "    if i == 4:\n",
    "        torch.save(model.state_dict(), f\"./models/bert_model_{i}\") # Saving the 4th model into storage (4th model is arbitrary, there is no reason that I chose 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating an aggregated confusion matrix based on the performance of the models in all 5 folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrix\n",
    "cm = confusion_matrix(true_labels_confusion, predicted_labels_confusion) # Creating the confusion matrix based on all the predicted labels across all 5 folds\n",
    "cm_normalized = cm.astype('float') / cm.sum(axis=1) # Normalized by row\n",
    "sns.heatmap(cm_normalized, annot=True, fmt='.5f', cmap='Blues', xticklabels=class_names, yticklabels=class_names)\n",
    "plt.xlabel('Predicted Labels')\n",
    "plt.ylabel('True Labels')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abstract = input() # Taking user input\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") # Selecting the device that will run the model (Preferably GPU)\n",
    "model = BERTClassifier(bert_model_name, num_classes).to(device) # Creating a model based on the predefined architecture\n",
    "model.load_state_dict(torch.load(\"models/bert_model_4\")) # Loading the saved weights\n",
    "model.eval() # Setting the model to evaluation mode for prediction \n",
    "\n",
    "# Tokenizing the input text ------------------------------------------------------------------------------------------------------------------\n",
    "tokenizer = BertTokenizer.from_pretrained(bert_model_name) # Tokenizer to tokenizer the input\n",
    "abstract_tokenized = tokenizer(abstract, return_tensors='pt', max_length=512, padding='max_length', truncation=True) # tokenizing the text for bert prediction\n",
    "input_ids = abstract_tokenized[\"input_ids\"].to(device) # Moving the inputs to the GPU (adjust this as you see fit)\n",
    "attention_mask = abstract_tokenized[\"attention_mask\"].to(device)\n",
    "\n",
    "# Predicting --------------------------------------------------------------------------------------------------------------------------\n",
    "with torch.no_grad():\n",
    "    logits = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "    prediction = torch.argmax(logits, dim=1).item()\n",
    "\n",
    "# Printing the output -----------------------------------------------------------------------------------------------------------------------------------------------\n",
    "print(f\"Input: {abstract}\")\n",
    "print(f\"-\"*50)\n",
    "print(f\"Academic field: {['Astronomy', 'Sociology', 'Psychology'][prediction]}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
